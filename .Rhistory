observation_prob <- list("listen" = matrix(c(0.85 , 0.15 ,
0.15 , 0.85) , nrow = 2 , byrow = TRUE) ,
"open-left" = "uniform" ,
"open-right" = "uniform")
reward <- data.frame("action" = c("listen" , "open-left" , "open-left" , "open-right" , "open-right") ,
"start-state" = c("*" , "tiger-left" , "tiger-right" , "tiger-left" , "tiger-right") ,
"end-state" = c("*" , "*" , "*" , "*" , "*") ,
"observation" = c("*" , "*" , "*" , "*" , "*") ,
"reward" = c(-1 , -100 , 10 , 10 , -100))
result <- pomdp(discount, states, actions, observations, start, transition_prob, observation_prob, reward, values = "reward", grid_size, verbose = FALSE)
plot(result)
install.packages("knitr")
library(knitr)
devtools::install(build_vignettes = TRUE)
vignette(igraph)
vignette("igraph")
edit(vignette("igraph"))
devtools::use_vignette("pomdp")
check(manual=TRUE)
getwd()
install.packages("texi2dvi")
tools::texi2dvi()
install.packages("Rd2md")
library("Rd2md", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.2")
Rd2pdf
Rd2pdf()
R CMD Rd2pdf
ReferenceManual(pkg = getwd() , outdir = getwd())
discount <- 0.75
values <- reward
states <- c("tiger-left" , "tiger-right")
actions <- c("listen" , "open-left" , "open-right")
observations <- c("tiger-left" , "tiger-right")
start <- "uniform"
grid_size <- 10
transition_prob <- list("listen" = "identity" ,
"open-left" = "uniform" ,
"open-right" = "uniform")
observation_prob <- list("listen" = matrix(c(0.85 , 0.15 ,
0.15 , 0.85) , nrow = 2 , byrow = TRUE) ,
"open-left" = "uniform" ,
"open-right" = "uniform")
reward <- data.frame("action" = c("listen" , "open-left" , "open-left" , "open-right" , "open-right") ,
"start-state" = c("*" , "tiger-left" , "tiger-right" , "tiger-left" , "tiger-right") ,
"end-state" = c("*" , "*" , "*" , "*" , "*") ,
"observation" = c("*" , "*" , "*" , "*" , "*") ,
"reward" = c(-1 , -100 , 10 , 10 , -100))
result <- pomdp(discount, states, actions, observations, start, transition_prob, observation_prob, reward, values = "reward", grid_size, verbose = FALSE)
result
result$pg
result
pomdp(discount, states, actions, observations, start, transition_prob, observation_prob, reward, values = "reward", grid_size, verbose = FALSE)
formals(print)
print.POMDP <- function(x, ...) {
cat("POMDP Object with...")
}
result
modelcomponents <- function(x) {
x$model
}
modelcomponents(result)
library(pomdp)
library(pomdp)
result
library(pomdp)
discount <- 0.75
values <- reward
states <- c("tiger-left" , "tiger-right")
actions <- c("listen" , "open-left" , "open-right")
observations <- c("tiger-left" , "tiger-right")
start <- "uniform"
grid_size <- 10
transition_prob <- list("listen" = "identity" ,
"open-left" = "uniform" ,
"open-right" = "uniform")
observation_prob <- list("listen" = matrix(c(0.85 , 0.15 ,
0.15 , 0.85) , nrow = 2 , byrow = TRUE) ,
"open-left" = "uniform" ,
"open-right" = "uniform")
reward <- data.frame("action" = c("listen" , "open-left" , "open-left" ,
"open-right" , "open-right") ,
"start-state" = c("*" , "tiger-left" , "tiger-right" ,
"tiger-left" , "tiger-right") ,
"end-state" = c("*" , "*" , "*" , "*" , "*") ,
"observation" = c("*" , "*" , "*" , "*" , "*") ,
"reward" = c(-1 , -100 , 10 , 10 , -100))
result <- pomdp(discount, states, actions, observations, start, transition_prob,
observation_prob, reward, values = "reward", grid_size, verbose = FALSE)
result
plot(result)
library(pomdp)
? vignettes
library(pomdp)
library(arules)
citation("pomdp")
discount <- 0.75
values <- "reward"
states <- c("tiger-left" , "tiger-right")
actions <- c("listen" , "open-left" , "open-right")
observations <- c("tiger-left" , "tiger-right")
start <- "uniform"
grid_size <- 10
transition_prob <- list("listen" = "identity" ,
"open-left" = "uniform" ,
"open-right" = "uniform")
observation_prob <- list("listen" = matrix(c(0.85 , 0.15 ,
0.15 , 0.85) , nrow = 2 , byrow = TRUE) ,
"open-left" = "uniform" ,
"open-right" = "uniform")
reward <- data.frame("action" = c("listen" , "open-left" , "open-left" ,
"open-right" , "open-right") ,
"start-state" = c("*" , "tiger-left" , "tiger-right" ,
"tiger-left" , "tiger-right") ,
"end-state" = c("*" , "*" , "*" , "*" , "*") ,
"observation" = c("*" , "*" , "*" , "*" , "*") ,
"reward" = c(-1 , -100 , 10 , 10 , -100))
result <- pomdp(discount, states, actions, observations, start, transition_prob,
observation_prob, reward, values = "reward", grid_size, verbose = FALSE)
result
print(result)
result
? structure
structure(1:6, dim = 2:3)
library(pomdp)
result
library(pomdp)
discount <- 0.75
values <- "reward"
states <- c("tiger-left" , "tiger-right")
actions <- c("listen" , "open-left" , "open-right")
observations <- c("tiger-left" , "tiger-right")
start <- "uniform"
grid_size <- 10
transition_prob <- list("listen" = "identity" ,
"open-left" = "uniform" ,
"open-right" = "uniform")
observation_prob <- list("listen" = matrix(c(0.85 , 0.15 ,
0.15 , 0.85) , nrow = 2 , byrow = TRUE) ,
"open-left" = "uniform" ,
"open-right" = "uniform")
reward <- data.frame("action" = c("listen" , "open-left" , "open-left" ,
"open-right" , "open-right") ,
"start-state" = c("*" , "tiger-left" , "tiger-right" ,
"tiger-left" , "tiger-right") ,
"end-state" = c("*" , "*" , "*" , "*" , "*") ,
"observation" = c("*" , "*" , "*" , "*" , "*") ,
"reward" = c(-1 , -100 , 10 , 10 , -100))
result <- pomdp(discount, states, actions, observations, start, transition_prob,
observation_prob, reward, values = "reward", grid_size, verbose = FALSE)
result
result
rm(list=ls())
discount <- 0.75
values <- "reward"
states <- c("tiger-left" , "tiger-right")
actions <- c("listen" , "open-left" , "open-right")
observations <- c("tiger-left" , "tiger-right")
start <- "uniform"
grid_size <- 10
transition_prob <- list("listen" = "identity" ,
"open-left" = "uniform" ,
"open-right" = "uniform")
observation_prob <- list("listen" = matrix(c(0.85 , 0.15 ,
0.15 , 0.85) , nrow = 2 , byrow = TRUE) ,
"open-left" = "uniform" ,
"open-right" = "uniform")
reward <- data.frame("action" = c("listen" , "open-left" , "open-left" ,
"open-right" , "open-right") ,
"start-state" = c("*" , "tiger-left" , "tiger-right" ,
"tiger-left" , "tiger-right") ,
"end-state" = c("*" , "*" , "*" , "*" , "*") ,
"observation" = c("*" , "*" , "*" , "*" , "*") ,
"reward" = c(-1 , -100 , 10 , 10 , -100))
result <- pomdp(discount, states, actions, observations, start, transition_prob,
observation_prob, reward, values = "reward", grid_size, verbose = FALSE)
result
model()
belief()
alpha()
pg()
reward()
initial.node()
library(pomdp)
# you need to use pomdp function to generate an object of class POMDP.
# here for an example we use the Tiger example code to generate such an object.
discount <- 0.75
values <- "reward"
states <- c("tiger-left" , "tiger-right")
actions <- c("listen" , "open-left" , "open-right")
observations <- c("tiger-left" , "tiger-right")
start <- "uniform"
grid_size <- 10
transition_prob <- list("listen" = "identity" ,
"open-left" = "uniform" ,
"open-right" = "uniform")
observation_prob <- list("listen" = matrix(c(0.85 , 0.15 ,
0.15 , 0.85) , nrow = 2 , byrow = TRUE) ,
"open-left" = "uniform" ,
"open-right" = "uniform")
reward <- data.frame("action" = c("listen" , "open-left" , "open-left" ,
"open-right" , "open-right") ,
"start-state" = c("*" , "tiger-left" , "tiger-right" ,
"tiger-left" , "tiger-right") ,
"end-state" = c("*" , "*" , "*" , "*" , "*") ,
"observation" = c("*" , "*" , "*" , "*" , "*") ,
"reward" = c(-1 , -100 , 10 , 10 , -100))
result <- pomdp(discount, states, actions, observations, start, transition_prob,
observation_prob, reward, values = "reward", grid_size, verbose = FALSE)
# then using the belief() function we print the all the belief states of the given POMDP object
belief(result)
library(pomdp)
library(pomdp)
source('~/pomdp-solve-master/Models/pomdp/R/belief.R')
rm(list = ls())
library(pomdp)
# you need to use pomdp function to generate an object of class POMDP.
# here for an example we use the Tiger example code to generate such an object.
discount <- 0.75
values <- "reward"
states <- c("tiger-left" , "tiger-right")
actions <- c("listen" , "open-left" , "open-right")
observations <- c("tiger-left" , "tiger-right")
start <- "uniform"
grid_size <- 10
transition_prob <- list("listen" = "identity" ,
"open-left" = "uniform" ,
"open-right" = "uniform")
observation_prob <- list("listen" = matrix(c(0.85 , 0.15 ,
0.15 , 0.85) , nrow = 2 , byrow = TRUE) ,
"open-left" = "uniform" ,
"open-right" = "uniform")
reward <- data.frame("action" = c("listen" , "open-left" , "open-left" ,
"open-right" , "open-right") ,
"start-state" = c("*" , "tiger-left" , "tiger-right" ,
"tiger-left" , "tiger-right") ,
"end-state" = c("*" , "*" , "*" , "*" , "*") ,
"observation" = c("*" , "*" , "*" , "*" , "*") ,
"reward" = c(-1 , -100 , 10 , 10 , -100))
result <- pomdp(discount, states, actions, observations, start, transition_prob,
observation_prob, reward, values = "reward", grid_size, verbose = FALSE)
# then using the belief() function we print the all the belief states of the given POMDP object
belief(result)
result
source('~/pomdp-solve-master/Models/pomdp/R/belief.proportions.R')
source('~/pomdp-solve-master/Models/pomdp/R/alpha.R')
source('~/pomdp-solve-master/Models/pomdp/R/pg.R')
source('~/pomdp-solve-master/Models/pomdp/R/total.expected.reward.R')
source('~/pomdp-solve-master/Models/pomdp/R/initial.node.R')
source('~/pomdp-solve-master/Models/pomdp/R/solver.output.R')
source('~/pomdp-solve-master/Models/pomdp/R/model.R')
library(pomdp)
# you need to use pomdp function to generate an object of class POMDP.
# here for an example we use the Tiger example code to generate such an object.
discount <- 0.75
values <- "reward"
states <- c("tiger-left" , "tiger-right")
actions <- c("listen" , "open-left" , "open-right")
observations <- c("tiger-left" , "tiger-right")
start <- "uniform"
grid_size <- 10
transition_prob <- list("listen" = "identity" ,
"open-left" = "uniform" ,
"open-right" = "uniform")
observation_prob <- list("listen" = matrix(c(0.85 , 0.15 ,
0.15 , 0.85) , nrow = 2 , byrow = TRUE) ,
"open-left" = "uniform" ,
"open-right" = "uniform")
reward <- data.frame("action" = c("listen" , "open-left" , "open-left" ,
"open-right" , "open-right") ,
"start-state" = c("*" , "tiger-left" , "tiger-right" ,
"tiger-left" , "tiger-right") ,
"end-state" = c("*" , "*" , "*" , "*" , "*") ,
"observation" = c("*" , "*" , "*" , "*" , "*") ,
"reward" = c(-1 , -100 , 10 , 10 , -100))
result <- pomdp(discount, states, actions, observations, start, transition_prob,
observation_prob, reward, values = "reward", grid_size, verbose = FALSE)
# then using the belief() function we print the all the belief states of the given POMDP object
belief(result)
library(pomdp)
library(pomdp)
library(pomdp)
library(pomdp)
discount <- 0.75
values <- "reward"
states <- c("tiger-left" , "tiger-right")
actions <- c("listen" , "open-left" , "open-right")
observations <- c("tiger-left" , "tiger-right")
start <- "uniform"
grid_size <- 10
transition_prob <- list("listen" = "identity" ,
"open-left" = "uniform" ,
"open-right" = "uniform")
observation_prob <- list("listen" = matrix(c(0.85 , 0.15 ,
0.15 , 0.85) , nrow = 2 , byrow = TRUE) ,
"open-left" = "uniform" ,
"open-right" = "uniform")
reward <- data.frame("action" = c("listen" , "open-left" , "open-left" ,
"open-right" , "open-right") ,
"start-state" = c("*" , "tiger-left" , "tiger-right" ,
"tiger-left" , "tiger-right") ,
"end-state" = c("*" , "*" , "*" , "*" , "*") ,
"observation" = c("*" , "*" , "*" , "*" , "*") ,
"reward" = c(-1 , -100 , 10 , 10 , -100))
result <- pomdp(discount, states, actions, observations, start, transition_prob,
observation_prob, reward, values = "reward", grid_size, verbose = FALSE)
result
alpha(result)
belief(result)
belief.proportions((result)
)
initial.node(result)
model(result)
? model
pg(result)
plot(result)
solver.output(result)
total.expected.reward(result)
library(pomdp)
library(pomdp)
library(pomdp)
library(pomdp)
library(pomdp)
detach("package:pomdp", unload=TRUE)
library("pomdp", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.2")
View(reward)
discount <- 0.75
values <- "reward"
states <- c("tiger-left" , "tiger-right")
actions <- c("listen" , "open-left" , "open-right")
observations <- c("tiger-left" , "tiger-right")
start <- "uniform"
grid_size <- 10
transition_prob <- list("listen" = "identity" ,
"open-left" = "uniform" ,
"open-right" = "uniform")
observation_prob <- list("listen" = matrix(c(0.85 , 0.15 ,
0.15 , 0.85) , nrow = 2 , byrow = TRUE) ,
"open-left" = "uniform" ,
"open-right" = "uniform")
reward <- data.frame("action" = c("listen" , "open-left" , "open-left" ,
"open-right" , "open-right") ,
"start-state" = c("*" , "tiger-left" , "tiger-right" ,
"tiger-left" , "tiger-right") ,
"end-state" = c("*" , "*" , "*" , "*" , "*") ,
"observation" = c("*" , "*" , "*" , "*" , "*") ,
"reward" = c(-1 , -100 , 10 , 10 , -100))
result <- pomdp(discount, states, actions, observations, start, transition_prob,
observation_prob, reward, values = "reward", grid_size, verbose = FALSE)
result
plot(result)
library(pomdp)
plot(result)
library(pomdp)
plot(result)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
?plot.igraph
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
?plot.igraph
plot(result)
library(pomdp)
discount <- 0.75
values <- "reward"
states <- c("tiger-left" , "tiger-right")
actions <- c("listen" , "open-left" , "open-right")
observations <- c("tiger-left" , "tiger-right")
start <- "uniform"
grid_size <- 10
transition_prob <- list("listen" = "identity" ,
"open-left" = "uniform" ,
"open-right" = "uniform")
observation_prob <- list("listen" = matrix(c(0.85 , 0.15 ,
0.15 , 0.85) , nrow = 2 , byrow = TRUE) ,
"open-left" = "uniform" ,
"open-right" = "uniform")
reward <- data.frame("action" = c("listen" , "open-left" , "open-left" ,
"open-right" , "open-right") ,
"start-state" = c("*" , "tiger-left" , "tiger-right" ,
"tiger-left" , "tiger-right") ,
"end-state" = c("*" , "*" , "*" , "*" , "*") ,
"observation" = c("*" , "*" , "*" , "*" , "*") ,
"reward" = c(-1 , -100 , 10 , 10 , -100))
result <- pomdp(discount, states, actions, observations, start, transition_prob,
observation_prob, reward, values = "reward", grid_size, verbose = FALSE)
# then using the alpha() function we print all the coefficients of the optimal solution of
# the given POMDP object
alpha(result)
vignette("POMDP")
vignette("pomdp")
library(pomdp)
library(pomdp)
vignette("pomdp")
source('~/pomdp-solve-master/Models/pomdp/R/model.R')
source('~/pomdp-solve-master/Models/pomdp/R/belief.R')
source('~/pomdp-solve-master/Models/pomdp/R/belief.proportions.R')
source('~/pomdp-solve-master/Models/pomdp/R/alpha.R')
source('~/pomdp-solve-master/Models/pomdp/R/pg.R')
source('~/pomdp-solve-master/Models/pomdp/R/total.expected.reward.R')
source('~/pomdp-solve-master/Models/pomdp/R/initial.node.R')
source('~/pomdp-solve-master/Models/pomdp/R/solver.output.R')
library(pomdp)
vignette("pomdp")
library(pomdp)
vignette("pomdp")
library(pomdp)
library(pomdp)
vignette("pomdp")
vignettes("pomdp")
vignette("pomdp")
source('~/pomdp-solve-master/Models/pomdp/R/model.R')
source('~/pomdp-solve-master/Models/pomdp/R/belief.R')
source('~/pomdp-solve-master/Models/pomdp/R/belief.proportions.R')
source('~/pomdp-solve-master/Models/pomdp/R/alpha.R')
source('~/pomdp-solve-master/Models/pomdp/R/pg.R')
source('~/pomdp-solve-master/Models/pomdp/R/total.expected.reward.R')
source('~/pomdp-solve-master/Models/pomdp/R/initial.node.R')
source('~/pomdp-solve-master/Models/pomdp/R/solver.output.R')
library(pomdp)
vignette("pomdp")
discount <- 0.75
values <- "reward"
states <- c("tiger-left" , "tiger-right")
actions <- c("listen" , "open-left" , "open-right")
observations <- c("tiger-left" , "tiger-right")
start <- "uniform"
grid_size <- 10
transition_prob <- list("listen" = "identity" ,
"open-left" = "uniform" ,
"open-right" = "uniform")
observation_prob <- list("listen" = matrix(c(0.85 , 0.15 ,
0.15 , 0.85) , nrow = 2 , byrow = TRUE) ,
"open-left" = "uniform" ,
"open-right" = "uniform")
reward <- data.frame("action" = c("listen" , "open-left" , "open-left" ,
"open-right" , "open-right") ,
"start-state" = c("*" , "tiger-left" , "tiger-right" ,
"tiger-left" , "tiger-right") ,
"end-state" = c("*" , "*" , "*" , "*" , "*") ,
"observation" = c("*" , "*" , "*" , "*" , "*") ,
"reward" = c(-1 , -100 , 10 , 10 , -100))
result <- pomdp(discount, states, actions, observations, start, transition_prob,
observation_prob, reward, values = "reward", grid_size, verbose = FALSE)
result
plot(result
)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
? igraph
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
plot(result)
x11()
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
x11()
plot(result)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/plot.POMDP.R', echo=TRUE)
plot(result)
source('~/pomdp-solve-master/Models/pomdp/R/model.R')
source('~/pomdp-solve-master/Models/pomdp/R/belief.R')
source('~/pomdp-solve-master/Models/pomdp/R/belief.proportions.R')
source('~/pomdp-solve-master/Models/pomdp/R/alpha.R')
source('~/pomdp-solve-master/Models/pomdp/R/pg.R')
source('~/pomdp-solve-master/Models/pomdp/R/total.expected.reward.R')
source('~/pomdp-solve-master/Models/pomdp/R/initial.node.R')
source('~/pomdp-solve-master/Models/pomdp/R/solver.output.R')
library(pomdp)
file.exists("~/.ssh/id_rsa.pub")
file.exists("~/.ssh/id_rsa.pub")
library(pomdp)
